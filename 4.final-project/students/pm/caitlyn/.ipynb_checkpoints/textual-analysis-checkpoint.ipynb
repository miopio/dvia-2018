{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual Analysis\n",
    "### Caitlyn Ralph\n",
    "### Free Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/caitlynr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import string\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer,TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions and global variables\n",
    "\n",
    "outputDir = \"/Users/caitlynr/Documents/school/dvia-2018/4.final-project/students/pm/caitlyn/output/\"\n",
    "path = \"/Users/caitlynr/Documents/school/dvia-2018/4.final-project/students/pm/caitlyn/songs/\"\n",
    "exclude = set(string.punctuation)\n",
    "ngramrange = (1,4)\n",
    "column = [\"song\"]\n",
    "\n",
    "# def remove_punctuation(s):\n",
    "#     stringCleaned = ''.join(ch for ch in s if ch not in exclude)\n",
    "#     return stringCleaned\n",
    "\n",
    "def process_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(',', ' ')\n",
    "    text = text.replace('/', ' ')\n",
    "    text = text.replace('(', ' ')\n",
    "    text = text.replace(')', ' ')\n",
    "    text = text.replace('.', ' ')\n",
    "    return text.split()\n",
    "\n",
    "def open_store(p,s):\n",
    "    for file in glob.glob(os.path.join(p, '*.txt')):\n",
    "        opened = open(file, \"r\")\n",
    "        text = opened.read()\n",
    "        s = s + \" \" + text\n",
    "    return s\n",
    "\n",
    "def open_store_album(p,d,s):\n",
    "    for file in d:\n",
    "        opened = open(p + file, \"r\")\n",
    "        text = opened.read()\n",
    "        s = s + \" \" + text\n",
    "    return s\n",
    "\n",
    "def open_store_clean(p,s):\n",
    "    s = open_store(p,s)\n",
    "#     sCleaned = remove_punctuation(s)\n",
    "    sCleaned = process_text(s)\n",
    "    return sCleaned\n",
    "\n",
    "def open_store_clean_album(p,d,s):\n",
    "    s = open_store_album(p,d,s)\n",
    "#     sCleaned = remove_punctuation(s)\n",
    "    sCleaned = process_text(s)\n",
    "    return sCleaned\n",
    "\n",
    "def generate_ngrams(wordsList, n):\n",
    "    ngramsList = []\n",
    "    for num in range(0, len(wordsList)):\n",
    "        ngram = ' '.join(wordsList[num:num + n])\n",
    "        ngramsList.append(ngram)\n",
    "    return ngramsList\n",
    "\n",
    "def counter_format(ngrams):\n",
    "    c = Counter(ngrams)\n",
    "    df = pd.DataFrame(list(c.items()), columns=['ngram', 'count'])\n",
    "    df = df.sort_values(by='count', ascending=False)\n",
    "    return df\n",
    "\n",
    "def create_list(p,l):\n",
    "    for file in glob.glob(os.path.join(p, '*.txt')):\n",
    "        opened = open(file, \"r\")\n",
    "        text = opened.readlines()\n",
    "        for line in text:\n",
    "            line = remove_punctuation(line)\n",
    "            l.append(line.strip())\n",
    "    return l\n",
    "\n",
    "def create_list_album(p,d,l):\n",
    "    for file in d:\n",
    "        opened = open(p + file, \"r\")\n",
    "        text = opened.readlines()\n",
    "        for line in text:\n",
    "            line = remove_punctuation(line)\n",
    "            l.append(line.strip())\n",
    "    return l\n",
    "\n",
    "def tfidf(df):\n",
    "    tvec = TfidfVectorizer(min_df=.0025, max_df=.1, stop_words='english', ngram_range=ngramrange)\n",
    "    tvecWeights = tvec.fit_transform(df.dropna())\n",
    "    weights = np.asarray(tvecWeights.mean(axis=0)).ravel().tolist()\n",
    "    weightsDF = pd.DataFrame({'term': tvec.get_feature_names(), 'weight': weights})\n",
    "    return weightsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. N-grams on full corpus of lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# open files and save text into one big string\n",
    "\n",
    "lyrics = \"\"\n",
    "\n",
    "lyricsCleaned = open_store_clean(path,lyrics)\n",
    "\n",
    "# tockenize and counter\n",
    "\n",
    "bigrams = generate_ngrams(lyricsCleaned,2)\n",
    "trigrams = generate_ngrams(lyricsCleaned,3)\n",
    "fourgrams = generate_ngrams(lyricsCleaned,4)\n",
    "fivegrams = generate_ngrams(lyricsCleaned,5)\n",
    "\n",
    "counter_format(bigrams).to_csv(\"bigrams.csv\")\n",
    "counter_format(trigrams).to_csv(\"trigrams.csv\")\n",
    "counter_format(fourgrams).to_csv(\"fourgrams.csv\")\n",
    "counter_format(fivegrams).to_csv(\"fivegrams.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. N-grams by album (\"era\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "# split songs into album lists\n",
    "\n",
    "epsPreAndEtc = [\"Anobrain.txt\",\"Antichrist.txt\",\"Facedown.txt\",\"Fallingforyou.txt\",\"Ghosts.txt\",\"Haunt    Bed.txt\",\n",
    "                \"Head.Cars.Bending.txt\",\"HNSCC.txt\",\"Intro Set3.txt\",\"Me.txt\",\"Medicine.txt\",\"Milk.txt\",\n",
    "                \"So Far (It's Alright).txt\",\"The 1975.txt\",\"Undo.txt\",\"Woman.txt\",\"You.txt\",\"102.txt\"]\n",
    "\n",
    "album1 = [\"Chocolate.txt\",\"Girls.txt\",\"Heart Out.txt\",\"Is There Somebody Who Can Watch You.txt\",\"M.O.N.E.Y..txt\",\n",
    "          \"Menswear.txt\",\"Pressure.txt\",\"Robbers.txt\",\"Settle Down.txt\",\"SEX.txt\",\"She Way Out.txt\",\"Talk!.txt\",\n",
    "          \"The City.txt\"]\n",
    "\n",
    "album2 = [\"A Change of Heart.txt\",\"How to Draw.txt\", \"I like it when you sleep, for you are so beautiful yet so unaware of it.txt\",\n",
    "          \"If I Believe You.txt\",\"Lostmyhead.txt\",\"Nana.txt\",\"Paris.txt\",\"She Lays Down.txt\",\"She's American.txt\",\n",
    "          \"Somebody Else.txt\",\"The Ballad of Me and My Brain.txt\",\"The Sound.txt\",\"This Must Be My Dream.txt\",\"UGH!.txt\",\"Love Me.txt\",\n",
    "         \"Loving Someone.txt\"]\n",
    "\n",
    "album3 = [\"A Brief Inquiry Into Online Relationships.txt\",\"Be My Mistake.txt\",\"Give Yourself a Try.txt\",\"How to Draw   Petrichor.txt\",\n",
    "          \"I Always Wanna Die (Sometimes).txt\",\"I Couldn't Be More in Love.txt\",\"I Like America & America Likes Me.txt\",\n",
    "          \"Inside Your Mind.txt\",\"It's Not Living (If It's Not With You).txt\",\"Love It If We Made It.txt\",\"Scary Monsters.txt\",\n",
    "          \"Sincerity Is Scary.txt\",\"Surrounded by Heads and Bodies.txt\",\"The Man Who Married a Robot   Love Theme.txt\",\n",
    "          \"TOOTIMETOOTIMETOOTIME.txt\",\"Mine.txt\"]\n",
    "\n",
    "# check that i got every song\n",
    "\n",
    "mySongs = []\n",
    "fileSongs = []\n",
    "\n",
    "myCount = 0\n",
    "fileCount = 0\n",
    "\n",
    "for song in epsPreAndEtc:\n",
    "    myCount+=1\n",
    "    mySongs.append(song)\n",
    "\n",
    "for song in album1:\n",
    "    myCount+=1\n",
    "    mySongs.append(song)\n",
    "\n",
    "for song in album2:\n",
    "    myCount+=1\n",
    "    mySongs.append(song)\n",
    "\n",
    "for song in album3:\n",
    "    myCount+=1\n",
    "    mySongs.append(song)\n",
    "\n",
    "for file in glob.glob(os.path.join(path, '*.txt')):\n",
    "    fileCount+=1\n",
    "    fileSongs.append(file.replace(path,\"\"))\n",
    "    \n",
    "print(myCount)\n",
    "print(fileCount)\n",
    "\n",
    "# print(fileSongs)\n",
    "# print(mySongs)\n",
    "\n",
    "for song in fileSongs:\n",
    "    if song not in mySongs:\n",
    "        print(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps, pre-albums, and other songs\n",
    "\n",
    "etcLyrics = \"\"\n",
    "\n",
    "etcLyricsCleaned = open_store_clean_album(path,epsPreAndEtc,etcLyrics)\n",
    "\n",
    "etcBigrams = generate_ngrams(etcLyricsCleaned,2)\n",
    "etcTrigrams = generate_ngrams(etcLyricsCleaned,3)\n",
    "etcFourgrams = generate_ngrams(etcLyricsCleaned,4)\n",
    "etcFivegrams = generate_ngrams(etcLyricsCleaned,5)\n",
    "\n",
    "counter_format(etcBigrams).to_csv(\"etc-bigrams.csv\")\n",
    "counter_format(etcTrigrams).to_csv(\"etc-trigrams.csv\")\n",
    "counter_format(etcFourgrams).to_csv(\"etc-fourgrams.csv\")\n",
    "counter_format(etcFivegrams).to_csv(\"etc-fivegrams.csv\")\n",
    "\n",
    "# album 1\n",
    "\n",
    "album1Lyrics = \"\"\n",
    "    \n",
    "album1LyricsCleaned = open_store_clean_album(path,album1,album1Lyrics)\n",
    "\n",
    "album1Bigrams = generate_ngrams(album1LyricsCleaned,2)\n",
    "album1Trigrams = generate_ngrams(album1LyricsCleaned,3)\n",
    "album1Fourgrams = generate_ngrams(album1LyricsCleaned,4)\n",
    "album1Fivegrams = generate_ngrams(album1LyricsCleaned,5)\n",
    "\n",
    "counter_format(album1Bigrams).to_csv(\"album1-bigrams.csv\")\n",
    "counter_format(album1Trigrams).to_csv(\"album1-trigrams.csv\")\n",
    "counter_format(album1Fourgrams).to_csv(\"album1-fourgrams.csv\")\n",
    "counter_format(album1Fivegrams).to_csv(\"album1-fivegrams.csv\")\n",
    "    \n",
    "# album 2\n",
    "\n",
    "album2Lyrics = \"\"\n",
    "\n",
    "album2LyricsCleaned = open_store_clean_album(path,album2,album2Lyrics)\n",
    "\n",
    "album2Bigrams = generate_ngrams(album2LyricsCleaned,2)\n",
    "album2Trigrams = generate_ngrams(album2LyricsCleaned,3)\n",
    "album2Fourgrams = generate_ngrams(album2LyricsCleaned,4)\n",
    "album2Fivegrams = generate_ngrams(album2LyricsCleaned,5)\n",
    "\n",
    "counter_format(album2Bigrams).to_csv(\"album2-bigrams.csv\")\n",
    "counter_format(album2Trigrams).to_csv(\"album2-trigrams.csv\")\n",
    "counter_format(album2Fourgrams).to_csv(\"album2-fourgrams.csv\")\n",
    "counter_format(album2Fivegrams).to_csv(\"album2-fivegrams.csv\")\n",
    "\n",
    "# album 3\n",
    "\n",
    "album3Lyrics = \"\"\n",
    "\n",
    "album3LyricsCleaned = open_store_clean_album(path,album3,album3Lyrics)\n",
    "\n",
    "album3Bigrams = generate_ngrams(album3LyricsCleaned,2)\n",
    "album3Trigrams = generate_ngrams(album3LyricsCleaned,3)\n",
    "album3Fourgrams = generate_ngrams(album3LyricsCleaned,4)\n",
    "album3Fivegrams = generate_ngrams(album3LyricsCleaned,5)\n",
    "\n",
    "counter_format(album3Bigrams).to_csv(\"album3-bigrams.csv\")\n",
    "counter_format(album3Trigrams).to_csv(\"album3-trigrams.csv\")\n",
    "counter_format(album3Fourgrams).to_csv(\"album3-fourgrams.csv\")\n",
    "counter_format(album3Fivegrams).to_csv(\"album3-fivegrams.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. TF-IDF on full corpus of lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to create a dataframe of our songs\n",
    "\n",
    "lyricsList = []\n",
    "lyricsList = create_list(path,lyricsList)\n",
    "# print(lyricsList)\n",
    "\n",
    "lyricsDF = pd.DataFrame(data=lyricsList,columns=column)\n",
    "# print(lyricsDF)\n",
    "\n",
    "# stemming\n",
    "\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "# lyricsDF['stemmed'] = lyricsDF.song.map(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n",
    "# lyricsDF.stemmed.head()\n",
    "\n",
    "# tf-idf\n",
    "\n",
    "result = tfidf(lyricsDF.song)\n",
    "result.sort_values(by='weight', ascending=False).to_csv(\"tfidf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. tf-idf by album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to create a dataframe of our songs\n",
    "\n",
    "etcList = []\n",
    "etcList = create_list_album(path,epsPreAndEtc,etcList)\n",
    "\n",
    "etcDF = pd.DataFrame(data=etcList,columns=column)\n",
    "\n",
    "etcResult = tfidf(etcDF.song)\n",
    "etcResult.sort_values(by='weight', ascending=False).to_csv(\"etc-tfidf.csv\")\n",
    "\n",
    "album1List = []\n",
    "album1List = create_list_album(path,album1,album1List)\n",
    "\n",
    "album1DF = pd.DataFrame(data=album1List,columns=column)\n",
    "\n",
    "album1Result = tfidf(album1DF.song)\n",
    "album1Result.sort_values(by='weight', ascending=False).to_csv(\"album1-tfidf.csv\")\n",
    "\n",
    "album2List = []\n",
    "album2List = create_list_album(path,album2,album2List)\n",
    "\n",
    "album2DF = pd.DataFrame(data=album2List,columns=column)\n",
    "\n",
    "album2Result = tfidf(album2DF.song)\n",
    "album2Result.sort_values(by='weight', ascending=False).to_csv(\"album2-tfidf.csv\")\n",
    "\n",
    "album3List = []\n",
    "album3List = create_list_album(path,album3,album3List)\n",
    "\n",
    "album3DF = pd.DataFrame(data=album3List,columns=column)\n",
    "\n",
    "album3Result = tfidf(album3DF.song)\n",
    "album3Result.sort_values(by='weight', ascending=False).to_csv(\"album3-tfidf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
